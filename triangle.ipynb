{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import multiprocessing\n",
    "import warnings;warnings.filterwarnings('ignore')\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from keras.preprocessing import sequence\n",
    "from keras import utils as np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout,Activation, Flatten\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters:\n",
    "cpu_count = multiprocessing.cpu_count()-1  #  cpu工作核数\n",
    "vocab_dim = 100\n",
    "n_iterations = 1                           #  W2V 随机梯度下降法中迭代的最大次数\n",
    "n_exposures = 2                            # 最小词频\n",
    "window_size = 5                            # 词向量上下文最大距离\n",
    "n_epoch = 4\n",
    "input_length = 110\n",
    "maxlen = 110\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_exposures = 7           最小词频           看一下字典大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file\n",
    "train_data = pd.read_csv('data/train.csv',lineterminator = '\\n')\n",
    "test_data  = pd.read_csv('data/test.csv',lineterminator = '\\n')\n",
    "    \n",
    "new_test_data  = test_data['review'].str.lower()\n",
    "testdata = new_test_data.as_matrix()                  #np.array(new_test_data,dtype=str)\n",
    "    \n",
    "new_train_data = train_data['review'].str.lower()\n",
    "traindata = new_train_data .as_matrix()            #np.array(new_train_data,dtype=str)\n",
    "\n",
    "\n",
    "train_data['num_label1'] = train_data.label.apply(lambda x:1 if x==\"Positive\" else 0 )\n",
    "y = train_data['num_label1'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer\n",
    "combined = [WordPunctTokenizer().tokenize(document.replace('\\n', '')) for document in traindata]\n",
    "test = [WordPunctTokenizer().tokenize(document.replace('\\n', '')) for document in testdata]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test[:2]\n",
    "[['yaqoob',\n",
    "  'memon',\n",
    "  'ki',\n",
    "  'phansi',\n",
    "  'zalimana',\n",
    "  'ghair',\n",
    "  'insani',\n",
    "  'hai',\n",
    "  '20',\n",
    "  'saal',\n",
    "  'qaid',\n",
    "  'kaat',\n",
    "  'chukay',\n",
    "  'thay',\n",
    "  'amnesty',\n",
    "  'international',\n",
    "  'ki',\n",
    "  'bharti',\n",
    "  'iqdam',\n",
    "  'ki',\n",
    "  'muzammat'],\n",
    " ['sabit', 'qadam', 'rehna']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionaries(model):\n",
    "\n",
    "    gensim_dict = Dictionary()\n",
    "    gensim_dict.doc2bow(model.wv.vocab.keys(), allow_update=True)\n",
    "        #gensim_dict.doc2bow(model.vocab.keys(),\n",
    "        #                    allow_update=True)\n",
    "        #  freqxiao10->0 所以k+1\n",
    "    w2indx = {v: k+1 for k, v in gensim_dict.items()}#超过频数的词语的索引,(k->v)=>(v->k)\n",
    "    w2vec = {word: model[word] for word in w2indx.keys()}#超过频数词语的词向量, (word->model(word))\n",
    "\n",
    "    print(len(w2indx))\n",
    "    print(len(w2indx))\n",
    "    return w2indx, w2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset(combined,index_dict):\n",
    "    data=[]\n",
    "    for sentence in combined:\n",
    "        new_txt = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                new_txt.append(index_dict[word])\n",
    "            except:\n",
    "                new_txt.append(0) # freqxiao10->0\n",
    "        data.append(new_txt)\n",
    "    return data # word=>index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7354\n",
      "7354\n"
     ]
    }
   ],
   "source": [
    "def word2vec_train(combined):\n",
    "\n",
    "    model = Word2Vec(size=vocab_dim, min_count=n_exposures, window=window_size,\n",
    "                     workers=cpu_count, iter=n_iterations)\n",
    "    model.build_vocab(combined)    # input: list\n",
    "    model.train(combined,total_examples=model.corpus_count, epochs=model.iter)#model.train(combined)\n",
    "    index_dict, word_vectors= create_dictionaries(model=model)\n",
    "    return   index_dict, word_vectors\n",
    "\n",
    "\n",
    "index_dict, word_vectors =word2vec_train(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined = parse_dataset(combined, index_dict)\n",
    "test = parse_dataset(test, index_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n=1\n",
    "[[0,\n",
    "  0,\n",
    "  2460,\n",
    "  3413,\n",
    "  0,\n",
    "  1532,\n",
    "  1951,\n",
    "  1643,\n",
    "  94,\n",
    "  3773,\n",
    "  3540,\n",
    "  0,\n",
    "  0,\n",
    "  4306,\n",
    "  0,\n",
    "  1966,\n",
    "  2460,\n",
    "  782,\n",
    "  0,\n",
    "  2460,\n",
    "  0],\n",
    " [3785, 3536, 3688]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n=2\n",
    "[[0,\n",
    "  0,\n",
    "  2460,\n",
    "  3413,\n",
    "  0,\n",
    "  1532,\n",
    "  1951,\n",
    "  1643,\n",
    "  94,\n",
    "  3773,\n",
    "  3540,\n",
    "  0,\n",
    "  0,\n",
    "  4306,\n",
    "  0,\n",
    "  1966,\n",
    "  2460,\n",
    "  782,\n",
    "  0,\n",
    "  2460,\n",
    "  0],\n",
    " [3785, 3536, 3688]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0, 4462, 3760, 5262,    0, 2355, 2991, 2542,  122, 5815,\n",
       "        5467,    0,    0, 6680,    0, 3014, 3760, 1172,    0, 3760,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0, 5835, 5460, 5682]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = sequence.pad_sequences(combined, \n",
    "                                  maxlen=maxlen)#每个句子所含词语对应的索引，所以句子中含有频数小于10的词语，索引为0\n",
    "test = sequence.pad_sequences(test, maxlen=maxlen)\n",
    "test[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(index_dict,word_vectors,combined,y):\n",
    "\n",
    "    n_symbols = len(index_dict) + 1        # 所有单词的索引数，频数小于10的词语索引为0，所以加1\n",
    "    embedding_weights = np.zeros((n_symbols, vocab_dim)) # 初始化 索引为0的词语，词向量全为0\n",
    "    for word, index in index_dict.items():               # 从索引为1的词语开始，对每个词语对应其词向量\n",
    "        embedding_weights[index, :] = word_vectors[word]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(combined, y, test_size=0)\n",
    "\n",
    "    y_train = np_utils.to_categorical(y_train,num_classes=2)\n",
    "    y_test = np_utils.to_categorical(y_test,num_classes=2)\n",
    "    # print (x_train.shape,y_train.shape)\n",
    "    return n_symbols,embedding_weights,x_train,y_train,x_test,y_test\n",
    "\n",
    "\n",
    "n_symbols,embedding_weights,x_train,y_train,x_test,y_test=get_data(index_dict, \n",
    "                                                                   word_vectors,combined,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 110, 100)          735500    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 765,802\n",
      "Trainable params: 765,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()  # or Graph or whatever\n",
    "\n",
    "model.add(Embedding(output_dim=vocab_dim,input_dim=n_symbols, mask_zero=True, weights=[embedding_weights],\n",
    "                    input_length=input_length))  # Adding Input Length\n",
    "\n",
    "model.add(LSTM(50, activation='tanh'))\n",
    "model.add(Dropout(0.3))                                   ##原0.5\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(2, activation='softmax')) # Dense=>全连接层,输出维度=3\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "#model.add(Activation('sigmoid'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "6328/6328 [==============================] - 30s 5ms/step - loss: 0.5752 - acc: 0.6991\n",
      "Epoch 2/4\n",
      "6328/6328 [==============================] - 29s 5ms/step - loss: 0.3214 - acc: 0.8662\n",
      "Epoch 3/4\n",
      "6328/6328 [==============================] - 24s 4ms/step - loss: 0.1900 - acc: 0.9313\n",
      "Epoch 4/4\n",
      "6328/6328 [==============================] - 26s 4ms/step - loss: 0.1241 - acc: 0.9559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a381141b38>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=batch_size, epochs = n_epoch, \n",
    "          verbose=1)# 0：不输出日志信息，1：输出进度条记录，2：每个epoch输出一行记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test,batch_size=batch_size)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lstm: relu  epoch =4 [0.5150559049741164, 0.7519747231620559]   \n",
    "lstm: relu  epoch =5 [0.8224221409591265, 0.7519747240095154]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lstm = model.predict_proba(test,batch_size=batch_size)[:,1]\n",
    "lstm_output = pd.DataFrame(data={\"ID\":test_data[\"ID\"], \"Pred\":y_lstm})\n",
    "lstm_output.to_csv('lstm_new.csv', index = False, quoting = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pd.Series.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
